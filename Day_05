###ğŸ“Œ Day 5: Gradient Descent  

Welcome to Day 5 of my 100 Consistent Days of Machine Learning challenge! ğŸš€  

Todayâ€™s focus is Gradient Descent, an essential optimization algorithm used to minimize the cost function in machine learning models.  


## ğŸ” What is Gradient Descent?  
Gradient Descent is an iterative optimization technique used to find the optimal parameters (weights and biases) that minimize the cost function (error/loss) in machine learning models.  

ğŸ’¡ Why is it important?  
- Helps models learn efficiently by updating parameters step by step.  
- Used in Linear Regression, Logistic Regression, Neural Networks, etc. 
- Essential for deep learning where the number of parameters is massive.  


## ğŸ“‰ How Does Gradient Descent Work?  
1ï¸âƒ£ Initialize Weights and Biases â€“ Start with random values.  
2ï¸âƒ£ Compute the Cost Function â€“ Measure how far predictions are from actual values.  
3ï¸âƒ£ Compute the Gradient (Derivative) â€“ Find the direction to move in order to reduce the error.  
4ï¸âƒ£ Update Parameters â€“ Adjust weights & biases using the formula:  
Î¸ = Î¸ - Î±.dJ/dÎ¸
where:  
- Î¸ = Model parameters (weights & biases)  
- Î± (alpha) = Learning rate (step size)  
- dJ/dÎ¸ = Gradient of the cost function  
5ï¸âƒ£ Repeat Until Convergence â€“ Keep updating until the error is minimized.  


## ğŸ›  Types of Gradient Descent  
âœ… Batch Gradient Descent (BGD) â€“ Uses the entire dataset for each update.  
âœ… Stochastic Gradient Descent (SGD) â€“ Uses a single data point per update (faster but noisier).  
âœ… Mini-Batch Gradient Descent â€“ Uses small batches of data for updates (balance between BGD & SGD).  


## ğŸ“ˆ Learning Rate & Challenges 
Choosing the right learning rate (Î±) is crucial:  
ğŸ”¹ Too large â†’ Jumps around, never converges.  
ğŸ”¹ Too small â†’ Slow learning, takes forever to converge.  
ğŸ”¹ Optimal â†’ Fast and smooth convergence.  

Common challenges:  
âš ï¸ Getting stuck in local minima (especially in non-convex functions).  
âš ï¸ Vanishing gradients in deep networks (solved using optimizers like Adam).  


## ğŸ“Œ Python Implementation  
Hereâ€™s a simple Gradient Descent implementation using NumPy:  

```python
import numpy as np

# Sample data (X: input, y: output)
X = np.array([1, 2, 3, 4, 5])
y = np.array([2, 4, 6, 8, 10])

# Initialize parameters
theta = 0  # Initial weight
alpha = 0.01  # Learning rate
iterations = 1000  # Number of updates

# Gradient Descent Algorithm
for i in range(iterations):
    gradient = np.dot(X, (np.dot(X, theta) - y)) / len(X)
    theta = theta - alpha * gradient  # Update weight

print(f"Optimized theta (weight): {theta}")
```


## ğŸš€ Key Takeaways  
âœ… Gradient Descent optimizes models by minimizing the cost function.  
âœ… Learning rate controls step size â€“ too large/small can cause issues.  
âœ… Used in almost all ML models, including deep learning.  

Thatâ€™s it for Day 5! Stay tuned for Day 6! ğŸš€
